{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch modules\n",
    "import os\n",
    "import numpy\n",
    "import time\n",
    "import string\n",
    "import warnings\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import vstack as vstack_sparse_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility functions\n",
    "\n",
    "def get_tokens(txt):\n",
    "    tokens=word_tokenize(txt)\n",
    "    return tokens\n",
    "\n",
    "def stemming(tokens, stemmer):\n",
    "    stemm =[]\n",
    "    for token in tokens:\n",
    "        stemm.append(stemmer.stem(token))\n",
    "    return stemm\n",
    "\n",
    "def preprocessing(txt):\n",
    "    tokens = get_tokens(txt)\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = stemming(tokens, stemmer)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'files1.txt', 'files2.txt', 'files3.txt', 'files4.txt']\n"
     ]
    }
   ],
   "source": [
    "#files for experiment\n",
    "print(os.listdir('.\\\\files'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model\n",
    "def train_Model():\n",
    "    fsList =[]\n",
    "    k = 0\n",
    "    path =\".\\\\files\"\n",
    "    vocabulary={}\n",
    "    \n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_path = subdir+os.path.sep+file\n",
    "            f=open(file_path, 'r')\n",
    "            small_case = f.read().lower()\n",
    "            skip_punct = small_case.translate(string.punctuation)\n",
    "            vocabulary[file] = skip_punct\n",
    "            fsList.append(file)\n",
    "    \n",
    "    tf_idf = TfidfVectorizer(tokenizer=preprocessing, stop_words = 'english')\n",
    "    corpus = vocabulary.values()\n",
    "    \n",
    "    tf_s = tf_idf.fit_transform(corpus)\n",
    "    \n",
    "    return tf_idf, tf_s, fsList    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 45)\t0.11398107370532325\n",
      "  (0, 26)\t0.0727526428822134\n",
      "  (0, 62)\t0.0727526428822134\n",
      "  (0, 159)\t0.2695919631306832\n",
      "  (0, 10)\t0.11398107370532325\n",
      "  (0, 28)\t0.11398107370532325\n",
      "  (0, 38)\t0.11398107370532325\n",
      "  (0, 60)\t0.11398107370532325\n",
      "  (0, 150)\t0.2279621474106465\n",
      "  (0, 112)\t0.11398107370532325\n",
      "  (0, 3)\t0.11398107370532325\n",
      "  (0, 143)\t0.11398107370532325\n",
      "  (0, 33)\t0.11398107370532325\n",
      "  (0, 39)\t0.11398107370532325\n",
      "  (0, 6)\t0.08986398771022773\n",
      "  (0, 161)\t0.11398107370532325\n",
      "  (0, 71)\t0.11398107370532325\n",
      "  (0, 36)\t0.11398107370532325\n",
      "  (0, 118)\t0.11398107370532325\n",
      "  (0, 107)\t0.11398107370532325\n",
      "  (0, 146)\t0.11398107370532325\n",
      "  (0, 131)\t0.11398107370532325\n",
      "  (0, 23)\t0.11398107370532325\n",
      "  (0, 63)\t0.11398107370532325\n",
      "  (0, 162)\t0.11398107370532325\n",
      "  :\t:\n",
      "  (3, 96)\t0.12412783640985797\n",
      "  (3, 86)\t0.12412783640985797\n",
      "  (3, 69)\t0.3723835092295739\n",
      "  (3, 147)\t0.24825567281971594\n",
      "  (3, 89)\t0.12412783640985797\n",
      "  (3, 136)\t0.24825567281971594\n",
      "  (3, 99)\t0.12412783640985797\n",
      "  (3, 79)\t0.12412783640985797\n",
      "  (3, 22)\t0.12412783640985797\n",
      "  (3, 47)\t0.12412783640985797\n",
      "  (3, 80)\t0.12412783640985797\n",
      "  (3, 115)\t0.12412783640985797\n",
      "  (3, 126)\t0.12412783640985797\n",
      "  (3, 70)\t0.12412783640985797\n",
      "  (3, 157)\t0.12412783640985797\n",
      "  (3, 90)\t0.12412783640985797\n",
      "  (3, 154)\t0.12412783640985797\n",
      "  (3, 155)\t0.12412783640985797\n",
      "  (3, 87)\t0.12412783640985797\n",
      "  (3, 7)\t0.09786381197347574\n",
      "  (3, 164)\t0.23768757023859602\n",
      "  (3, 26)\t0.23768757023859602\n",
      "  (3, 62)\t0.07922919007953201\n",
      "  (3, 25)\t0.19432516321016613\n",
      "  (3, 24)\t0.12955010880677742\n"
     ]
    }
   ],
   "source": [
    "tf_idf, tf_s, files= train_Model()\n",
    "print(tf_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File is plagarized with files3.txt by 100.00000000000007\n",
      "time taken  0.013982772827148438\n"
     ]
    }
   ],
   "source": [
    "#verifying the input file\n",
    "f=open(\".\\\\input.txt\")\n",
    "files.append(\"input.txt\")\n",
    "\n",
    "start_ts = time.time()\n",
    "txt=f.read().lower().translate(string.punctuation)\n",
    "resp=tf_idf.transform([txt])\n",
    "tf_s_combo = vstack_sparse_matrices([tf_s,resp])\n",
    "m=(tf_s_combo*tf_s_combo.T).A\n",
    "number_of_rows = len(m)\n",
    "for f in range(0, len(m[0])-1):\n",
    "    if (m[number_of_rows-1,f]) >0.8:\n",
    "        print(f\"File is plagarized with {files[f]} by {m[number_of_rows-1][f]* 100 }\")\n",
    "\n",
    "end_ts = time.time()\n",
    "print(\"time taken \", end_ts-start_ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
